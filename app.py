# -*- coding: utf-8 -*-
# Agente de An√°lise de Dados e Detec√ß√£o de Fraudes com Gemini SDK (Vers√£o Final Est√°vel)
# Elimina a LangChain para resolver erros de Output Parsing e Deprecia√ß√£o.

import streamlit as st
import pandas as pd
import os
import tempfile
import zipfile
import gzip
import io
import json
import plotly.express as px
import plotly.io as pio
import google.generativeai as genai
import traceback

# --- Configura√ß√µes Iniciais do Streamlit ---
st.set_page_config(layout="wide", page_title="Multi Agente de An√°lise Fiscal e de Fraudes (Est√°vel)")

# --- Constantes e Vari√°veis Globais ---
pio.templates.default = "plotly_white"
MODEL_NAME = "gemini-2.5-flash"

# Tenta obter a chave da API do Gemini do secrets.toml (Streamlit Cloud)
try:
    API_KEY = st.secrets["GEMINI_API_KEY"]
except (KeyError, AttributeError):
    API_KEY = os.environ.get("GEMINI_KEY", "")

if not API_KEY:
    st.error("ERRO: Chave da API do Gemini n√£o encontrada. Configure a chave no .streamlit/secrets.toml ou na vari√°vel de ambiente GEMINI_KEY.")

# --- Inicializa√ß√£o Est√°vel do Gemini ---

@st.cache_resource
def get_gemini_client(api_key, model_name):
    """
    Inicializa e armazena o cliente Gemini na cache para evitar consumo de cota.
    A temperatura (temperature) √© definida APENAS na chamada generate_content.
    """
    if not api_key:
        return None
    try:
        genai.configure(api_key=api_key)
        client = genai.GenerativeModel(
            model_name=model_name
            # 'temperature' √© passada na chamada para evitar erros de compatibilidade do SDK
        )
        return client
    except Exception as e:
        st.error(f"Erro fatal ao configurar o Gemini SDK. Verifique sua chave de API. Detalhes: {e}")
        return None

# Chame a fun√ß√£o cacheada para obter o cliente
gemini_client = get_gemini_client(API_KEY, MODEL_NAME)

# --- Fun√ß√µes de Manipula√ß√£o de Arquivos ---

@st.cache_data
def unzip_and_read_file(uploaded_file):
    """
    Descompacta arquivos ZIP ou GZ, l√™ o conte√∫do CSV e retorna o DataFrame
    e o caminho tempor√°rio do arquivo (necess√°rio para o contexto do prompt).
    """
    file_extension = uploaded_file.name.lower().split('.')[-1]
    uploaded_file.seek(0)
    
    # Cria o arquivo tempor√°rio de forma s√≠ncrona
    with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp_file:
        tmp_csv_path = tmp_file.name

    try:
        # L√≥gica de descompacta√ß√£o e leitura
        if file_extension == 'zip':
            with zipfile.ZipFile(uploaded_file, 'r') as zf:
                csv_files = [name for name in zf.namelist() if name.endswith('.csv')]
                if not csv_files:
                    st.error("Nenhum arquivo CSV encontrado dentro do ZIP.")
                    return None, None
                
                with zf.open(csv_files[0]) as csv_file:
                    df = pd.read_csv(csv_file)
                    df.to_csv(tmp_csv_path, index=False)
                    return tmp_csv_path, df
        
        elif uploaded_file.name.endswith(('.gz', '.gzip')):
            with gzip.open(uploaded_file, 'rt') as gz_file:
                df = pd.read_csv(gz_file)
                df.to_csv(tmp_csv_path, index=False)
                return tmp_csv_path, df

        elif file_extension == 'csv':
            df = pd.read_csv(uploaded_file)
            uploaded_file.seek(0)
            with open(tmp_csv_path, 'wb') as f:
                f.write(uploaded_file.read())
            return tmp_csv_path, df
    
    except Exception as e:
        st.error(f"Erro ao processar o arquivo: {e}")
        if os.path.exists(tmp_csv_path):
            os.remove(os.path.abspath(tmp_csv_path)) # Garante a remo√ß√£o
        return None, None
    
    return tmp_csv_path, df

def get_specialist_prompt(df, temp_csv_path):
    """Gera o prompt de sistema para instruir o Gemini como especialista."""
    # CORRE√á√ÉO CR√çTICA: Enviar APENAS metadados (colunas e tipos) para economizar tokens/tempo
    col_info = df.dtypes.to_markdown()
    
    return f"""
    Voc√™ √© um Multi Agente de IA SUPER ESPECIALISTA em Contabilidade, An√°lise de Dados e Desenvolvimento Python.
    Sua miss√£o √© analisar dados do arquivo CSV localizado em: {temp_csv_path}.

    **Contexto do Arquivo:**
    - O arquivo possui {df.shape[0]} linhas e {df.shape[1]} colunas.
    - **Tipos de Dados:**
    {col_info}

    **Regras OBRIGAT√ìRIAS (Essencial para a estabilidade e gr√°ficos):**
    1. **Ferramenta √önica:** Voc√™ tem acesso √† biblioteca Pandas.
    2. **Sa√≠da de Gr√°fico:** **SEMPRE** que o usu√°rio solicitar uma visualiza√ß√£o, gere o c√≥digo Python completo usando **Plotly Express (px)**.
    3. **Formato:** O c√≥digo Python para o gr√°fico deve ser **impresso** no formato de string JSON do Plotly, usando o comando:
       `print(f"<PLOTLY_JSON>{{fig.to_json()}}</PLOTLY_JSON>")`
    4. **Caminho do Arquivo:** **SEMPRE** use `pd.read_csv('{temp_csv_path}')` dentro do c√≥digo Python que voc√™ gerar.
    5. **Evitar Quebra:** Mantenha as respostas focadas. N√£o use racioc√≠nio em etapas ou comandos internos do LangChain que causam erros de parsing.
    6. **Resumo:** Ao final da sua an√°lise ou do c√≥digo, forne√ßa um resumo claro e conciso da sua conclus√£o.
    """

def execute_python_code(code_str, temp_csv_path):
    """Executa c√≥digo Python gerado pelo LLM em um ambiente seguro."""
    # Define o ambiente de execu√ß√£o com o dataframe lido
    exec_globals = {
        'pd': pd,
        'px': px,
        'plt': None, # Remove matplotlib
        'df': pd.read_csv(temp_csv_path),
        'print': print # Permite que o LLM use print para comunica√ß√£o
    }
    
    # Prepara um buffer para capturar a sa√≠da do print
    output_buffer = io.StringIO()
    
    try:
        # Redireciona a sa√≠da padr√£o (stdout) para o nosso buffer
        import sys
        sys.stdout = output_buffer
        
        # Executa o c√≥digo. O c√≥digo deve usar 'df'
        exec(code_str, exec_globals)
        
        # Restaura a sa√≠da padr√£o
        sys.stdout = sys.__stdout__
        
        # Retorna a sa√≠da capturada (incluindo tags JSON)
        return output_buffer.getvalue()
        
    except Exception as e:
        sys.stdout = sys.__stdout__ # Garantir que o stdout seja restaurado
        return f"ERRO DE EXECU√á√ÉO PYTHON: {e}\nTraceback: {traceback.format_exc()}"

def parse_and_display_response(response_text):
    """
    Analisa a resposta, extrai e executa c√≥digo Python e renderiza o gr√°fico/texto.
    """
    CODE_START = "```python"
    CODE_END = "```"
    PLOTLY_TAG = "<PLOTLY_JSON>"

    if CODE_START in response_text:
        # Separa o texto e o c√≥digo
        parts = response_text.split(CODE_START, 1)
        text_before = parts[0].strip()
        
        try:
            code_block = parts[1].split(CODE_END, 1)[0].strip()
            text_after = parts[1].split(CODE_END, 1)[1].strip() if len(parts[1].split(CODE_END, 1)) > 1 else ""
        except IndexError:
            # Caso o c√≥digo Python esteja no final e n√£o tenha o fechamento ```
            code_block = parts[1].strip()
            text_after = ""

        # Executa o c√≥digo Python
        execution_output = execute_python_code(code_block, st.session_state.temp_csv_path)
        
        # Exibe o texto explicativo antes do c√≥digo (se houver)
        if text_before:
            st.chat_message("assistant").markdown(text_before)

        # Verifica se o c√≥digo gerou JSON Plotly (via print)
        if PLOTLY_TAG in execution_output:
            try:
                # Extrai o JSON usando as tags
                json_str = execution_output.split(PLOTLY_TAG, 1)[1].split("</PLOTLY_JSON>", 1)[0].strip()
                fig_dict = json.loads(json_str)
                fig = pio.from_json(json.dumps(fig_dict))
                
                # Renderiza o gr√°fico
                st.plotly_chart(fig, use_container_width=True)
                
                st.chat_message("assistant").markdown("‚úÖ **Gr√°fico gerado:** Analise a visualiza√ß√£o acima.")
                
            except Exception as e:
                # Se falhar ao decodificar o JSON, exibe o erro
                st.chat_message("assistant").error(f"‚ö†Ô∏è Falha ao renderizar o gr√°fico. Erro de JSON: {e}")
                
        elif "ERRO DE EXECU√á√ÉO PYTHON" in execution_output:
            # Exibe erro de execu√ß√£o
            st.chat_message("assistant").error(f"‚ùå Erro ao executar c√≥digo Python: {execution_output}")
        
        # Exibe o texto explicativo ap√≥s o c√≥digo (se houver)
        if text_after:
            st.chat_message("assistant").markdown(text_after)
            
    else:
        # Se n√£o h√° c√≥digo Python, exibe a resposta como texto simples
        st.chat_message("assistant").markdown(response_text)

# --- Layout e Interface ---

st.title("ü§ñ Multi Agente de An√°lise Fiscal e de Fraudes")
st.markdown("---")

# Inicializa√ß√£o de estado
if 'temp_csv_path' not in st.session_state:
    st.session_state.temp_csv_path = None
if 'df' not in st.session_state:
    st.session_state.df = None
if 'chat_history_list' not in st.session_state:
    st.session_state.chat_history_list = []
if 'report_content' not in st.session_state:
    st.session_state.report_content = ""
if 'specialist_prompt' not in st.session_state:
    st.session_state.specialist_prompt = ""
if 'temp_csv_path_to_delete' not in st.session_state:
    st.session_state.temp_csv_path_to_delete = None


# --- Barra Lateral (Upload e Relat√≥rio) ---
with st.sidebar:
    st.header("‚öôÔ∏è Configura√ß√µes de An√°lise")
    
    uploaded_file = st.file_uploader(
        "Carregue seu arquivo CSV, ZIP ou GZ:",
        type=['csv', 'zip', 'gz'],
        key="file_uploader"
    )
    
    st.subheader("Relat√≥rio Final")
    report_btn = st.button("üìù Gerar Conclus√£o da An√°lise", use_container_width=True)
    
    if st.session_state.report_content:
        st.download_button(
            label="‚¨áÔ∏è Baixar Relat√≥rio (Markdown)",
            data=st.session_state.report_content,
            file_name="relatorio_analise_ia.md",
            mime="text/markdown",
            use_container_width=True
        )

# --- Processamento do Arquivo ---

if uploaded_file and st.session_state.temp_csv_path is None:
    st.session_state.temp_csv_path, st.session_state.df = unzip_and_read_file(uploaded_file)
    
    if st.session_state.df is not None:
        st.session_state.specialist_prompt = get_specialist_prompt(st.session_state.df, st.session_state.temp_csv_path)
        st.session_state.chat_history_list.clear() # Limpa o chat ao carregar novo arquivo
        st.success(f"Arquivo '{uploaded_file.name}' carregado e pronto para an√°lise! Pergunte no chat.")
    else:
        st.error("Falha ao carregar o arquivo. Verifique o formato.")

# --- Processamento do Relat√≥rio ---

if report_btn and st.session_state.df is not None:
    report_prompt = st.session_state.specialist_prompt + "\n\nFa√ßa uma conclus√£o resumida e completa de toda a an√°lise de dados realizada at√© agora, incorporando as se√ß√µes: Resumo Executivo, Detalhes da An√°lise, e Conclus√£o Final. Sua resposta deve ser SOMENTE o conte√∫do do relat√≥rio em Markdown."
    
    history_context = "\n".join([f"{h['role']}: {h['content']}" for h in st.session_state.chat_history_list])
    
    full_prompt = report_prompt + "\n\nHist√≥rico da Conversa:\n" + history_context

    with st.spinner("Gerando relat√≥rio completo..."):
        try:
            response = gemini_client.generate_content(
                full_prompt, 
                config={"temperature": 0.0, "timeout": 180} # Configura√ß√£o de precis√£o e timeout
            )
            st.session_state.report_content = response.text
            st.success("Relat√≥rio gerado com sucesso! Use o bot√£o 'Baixar Relat√≥rio (Markdown)' na lateral.")
            
        except Exception as e:
            st.error(f"Erro ao gerar o relat√≥rio: {e}")

# --- Interface de Chat ---

# Exibe o hist√≥rico de chat
for item in st.session_state.chat_history_list:
    role = item['role']
    content = item['content']
    
    if role == "user":
        st.chat_message("user").markdown(content)
    else:
        # O parser agora est√° integrado na fun√ß√£o principal
        parse_and_display_response(content)


# Campo de entrada de prompt do usu√°rio
if st.session_state.df is not None and gemini_client:
    if prompt := st.chat_input("Fa√ßa sua pergunta ao Agente..."):
        # Adiciona a pergunta ao hist√≥rico
        st.session_state.chat_history_list.append({"role": "user", "content": prompt})
        st.chat_message("user").markdown(prompt)

        with st.spinner("Agente de IA est√° processando..."):
            try:
                # Constr√≥i o contexto da conversa
                history_context = "\n".join([f"{item['role']}: {item['content']}" for item in st.session_state.chat_history_list])
                full_context = st.session_state.specialist_prompt + "\n\n" + history_context

                # Chama a API do Gemini com o contexto completo
                response = gemini_client.generate_content(
                    full_context,
                    config={"temperature": 0.0, "timeout": 180} # Configura√ß√£o de precis√£o e timeout
                )
                response_text = response.text
                
                # Adiciona a resposta completa ao hist√≥rico
                st.session_state.chat_history_list.append({"role": "assistant", "content": response_text})

                # Processa e exibe a resposta (incluindo c√≥digo/gr√°fico)
                parse_and_display_response(response_text)

            except Exception as e:
                st.session_state.chat_history_list.append({"role": "assistant", "content": "Ocorreu um erro na comunica√ß√£o com a IA. Por favor, tente novamente ou reformule sua pergunta."})
                st.chat_message("assistant").error("‚ùå Erro de comunica√ß√£o ou timeout. Tente novamente.")
                print(f"Erro na execu√ß√£o da API: {e}")

# Footer
if st.session_state.df is None:
    st.info("‚ö†Ô∏è Carregue um arquivo CSV, ZIP ou GZ para iniciar a an√°lise.")
